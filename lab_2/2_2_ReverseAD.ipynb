{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef88843b66adc91569384659597bfa0d",
     "grade": false,
     "grade_id": "cell-3f061ea27f528fcf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 3: Reverse Mode Automatic Differentiation\n",
    "\n",
    "Dynamic Reverse mode AD can be implemented by declaring a class to represent a value and the child expressions that the value depends on. We've provided the implementation that was shown in the lecture slides as a basis below, but it's missing some parts that will make it useful.\n",
    "\n",
    "__Tasks:__\n",
    "\n",
    "- Addition (`__add__`) is incomplete - can you finish it? \n",
    "- Can you also implement division (`__truediv__`), subtraction (`__sub__`) and power (`__pow__`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Var:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.grad_value = None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.grad_value is None:\n",
    "            self.grad_value = sum(weight * var.grad()\n",
    "                                  for weight, var in self.children)\n",
    "        return self.grad_value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        z = Var(self.value * other.value)\n",
    "        self.children.append((other.value, z))\n",
    "        other.children.append((self.value, z))\n",
    "        return z\n",
    "\n",
    "    def __add__(self, other):\n",
    "        z = Var(self.value + other.value)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((1.0, z))\n",
    "        return z\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        z = Var(self.value - other.value)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((-1.0, z))\n",
    "        return z\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        z = Var(self.value / other.value)\n",
    "        self.children.append((1 / other.value, z))\n",
    "        other.children.append((- self.value / (other.value ** 2), z))\n",
    "        return z\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        z = Var(self.value ** other.value)\n",
    "        self.children.append((other.value * (self.value ** (other.value - 1)), z))\n",
    "        other.children.append((math.log(self.value) * (self.value ** other.value), z))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.Var at 0x285f243a388>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "Var(1) + Var(1) / Var(1) - Var(1)**Var(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "136e5e779ef4c951e75380a1554d1543",
     "grade": false,
     "grade_id": "cell-7a8d45cf51fc131f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing math functions\n",
    "\n",
    "Just like when we were looking at Forward Mode AD, we also need to implement some core math functions. Here's the sine function for a `Var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    z = Var(math.sin(x.value))\n",
    "    x.children.append((math.cos(x.value), z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f733095db7ef9f78d62daf4d675492d3",
     "grade": false,
     "grade_id": "cell-71185787c3ab6312",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "__Task:__ can you implement the _cosine_ (`cos`), _tangent_ (`tan`), and _exponential_ (`exp`) functions in the code block below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(x):\n",
    "    z = Var(math.cos(x.value))\n",
    "    x.children.append((-math.sin(x.value), z))\n",
    "    return z\n",
    "\n",
    "def tan(x):\n",
    "    # YOUR CODE HERE\n",
    "    z = Var(math.tan(x.value))\n",
    "    x.children.append((math.tan(x.value) ** 2, z))\n",
    "    return z\n",
    "\n",
    "def exp(x):\n",
    "    z = Var(math.exp(x.value))\n",
    "    x.children.append((math.exp(x.value), z))\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert cos(Var(0)).value == 1\n",
    "assert tan(Var(0)).value == 0\n",
    "assert exp(Var(0)).value == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to try it out\n",
    "\n",
    "We're now in a position to try our implementation.\n",
    "\n",
    "__Tasks:__ \n",
    "\n",
    "- Try running the following code to compute the value of the function $z=x\\cdot y+sin(x)$ given $x=0.5$ and $y=4.2$, together with the derivative $\\partial z/\\partial x$ at that point. \n",
    "- Verify that the result is correct by hand-differentiating the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "z: 2.579425538604203\n∂z/∂x: 5.077582561890373\n"
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = x * y + sin(x)\n",
    "print('z:', z)\n",
    "\n",
    "z.grad_value = 1.0 #Note that we have to 'seed' the gradient of z to 1 (e.g. ∂z/∂z=1) before computing grads\n",
    "print('∂z/∂x:',x.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ Now use the code block below to compute the derivative $\\partial z/\\partial y$ of the above expression (at the same point $x=0.5, y=4.2$ as above). Store the resultant gradient in the variable `dzdy`. Verify by hand that the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "∂z/∂y: 0.5\n"
    }
   ],
   "source": [
    "\n",
    "dzdy = y.grad()\n",
    "print('∂z/∂y:', dzdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dzdy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating Algorithms\n",
    "\n",
    "Now, let's look at doing something wacky: differentiate an algorithm. For this example, we'll use an algorithm that is in a sense static (in this particular case the upper limit of the for loop is predetermined). However, it is not difficult to see that AD is much more general, and could even be applied to stochastic algorithms (say if we replaced the upper limit of the loop below with `Math.floor(Math.random() * 10)` for example).\n",
    "\n",
    "__Task:__ Consider the following algorithm and in the box below it manually compute the value of $z$ and the gradient $\\partial z/\\partial x$ at the end of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(0.5)\n",
    "z = Var(1)\n",
    "for i in range(0,2):\n",
    "    z = (z + Var(i)) * x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5058b059e97e150c869316d58034822",
     "grade": true,
     "grade_id": "cell-b0d4a79348257124",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$z = (((1 + 0) * x * x) + 1) * x * x$\n",
    "$z = (x^2 + 1) * x * x$\n",
    "$z = x^4 + x^2$\n",
    "$z = 0.3125$\n",
    "\n",
    "$\\frac{\\delta z}{\\delta x} = 4x^3 + 2x = 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task__: Now use the code block below to print out the gradient computed by our reverse AD by storing the result in a variable called `grad`. Does it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1.5\n"
    }
   ],
   "source": [
    "z.grad_value = 1.0\n",
    "grad = x.grad()\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ Finally, use the code block below to experiment and test the other math functions and methods you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}