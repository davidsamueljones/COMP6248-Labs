{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "bx-ZeOJAQGQS",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7142079a02673557fd4d7c62d09fb35",
     "grade": false,
     "grade_id": "cell-59e5add4db8ca4cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Optimising functions\n",
    "    \n",
    "In this lab we will play with some of the optimisation methods we learned in the lecture by exploring how they work on some analytic functions (both convex and non-convex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "cG3yqv0BQQVx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6d5a1e2037a78f4a83c58ba02f204b6",
     "grade": false,
     "grade_id": "cell-3c9fb8c6c798fa3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A Simple Function\n",
    "\n",
    "For this first task, we are going to try to optimise the following using Stochastic Gradient Descent:\n",
    "\n",
    "\\begin{equation}\n",
    "min_{\\textbf{x}} (\\textbf{x}[0] - 5)^2 + \\textbf{x}[1]^2 + (\\textbf{x}[2] - 1)^2\\; ,\n",
    "\\end{equation}\n",
    "\n",
    "Use the following block the write down the analytic minima of the above function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086b733ee28421cd24eb19ade35c6ed4",
     "grade": true,
     "grade_id": "cell-b78040990d924cd0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "x[0] = 5\n",
    "x[1] = 0\n",
    "x[2] = 1\n",
    "Min = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62407a53e2c8038e31413b6e939f397c",
     "grade": false,
     "grade_id": "cell-31897742431a78ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Implement the function\n",
    "\n",
    "First, complete the following code block to implement the above function using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return (x[0] - 5) ** 2 + (x[1] ** 2) + (x[2] - 1) ** 2\n",
    "\n",
    "def Fibonacci(n):\n",
    "    if n<0: \n",
    "        raise ValueError()\n",
    "    # First Fibonacci number is 0 \n",
    "    elif n==0: \n",
    "        return torch.tensor(0.0, requires_grad=True)\n",
    "    elif n==1: \n",
    "        return torch.tensor(1.0, requires_grad=True)\n",
    "    else: \n",
    "        return Fibonacci(n-1)+Fibonacci(n-2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "hfTJ2GI_Qcej",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7480042c8d67ef7613ee4bfea5d4971c",
     "grade": false,
     "grade_id": "cell-3216df41fb235ae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Optimising\n",
    "\n",
    "We need two more things before we can start optimising.\n",
    "We need our initial guess - which we've set to [2.0, 1.0, 10.0] and we need to how many epochs to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor(4.0, requires_grad=True)\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "nGNScu9QQgbZ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1de659ee2ace2ae86f8601c28e1033e7",
     "grade": false,
     "grade_id": "cell-a71c41511464a13e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We define the optimisation loop in the standard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(4., requires_grad=True)\nNone\n"
    }
   ],
   "source": [
    "p = torch.tensor(4.0, requires_grad=True)\n",
    "p.retain_grad()\n",
    "opt = optim.SGD([p], lr=0.01)\n",
    "output = Fibonacci(p)\n",
    "output.backward()\n",
    "print(p)\n",
    "print(output.grad)\n",
    "\n",
    "for i in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    output = Fibonacci(p)\n",
    "    output.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "hFXAQAXqQi3H",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55c0aa9e2acd143f369fd00849faa6c5",
     "grade": false,
     "grade_id": "cell-f63a8e54eca095d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following block to print out the final value of `p`. Does it match the value you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(4., requires_grad=True)\nNone\n"
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Himmelblau's Function\n",
    "\n",
    "We'll now have a go at a more complex example, which we also visualise, with multiple optima; [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function). This is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\\; ,\n",
    "\\end{equation}\n",
    "and has minima at\n",
    "\\begin{equation}\n",
    "f(3, 2) = f(-2.805118, 3.131312) = f(-3.779310, -3.283186) = f(3.584428, -1.848126) = 0\\; .\n",
    "\\end{equation}\n",
    "\n",
    "Use the following block to first define the function (the inputs $x, y$ are packed into a vector as for the previous quadratic function above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himm(x):\n",
    "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a87861c51de11fc596ef3de86b950c70",
     "grade": false,
     "grade_id": "cell-fc81ef599d739b56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following will plot its surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "xmin, xmax, xstep = -5, 5, .2\n",
    "ymin, ymax, ystep = -5, 5, .2\n",
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "z = himm(torch.tensor([x, y])).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
    "ax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b6e3379a1d9fb21879d0845ca7ba072",
     "grade": false,
     "grade_id": "cell-f938244581085c83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check that the above plot looks correct  by comparing to the picture on the [Wikipedia page](https://en.wikipedia.org/wiki/Himmelblau%27s_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "198bab34933b991f896a519e2f4162fd",
     "grade": false,
     "grade_id": "cell-574019e663811dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Optimising\n",
    "\n",
    "Let's see how it looks for a few different optimisers from a range of starting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, xstep = -5, 5, .2\n",
    "ymin, ymax, ystep = -5, 5, .2\n",
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "z = himm(torch.tensor([x, y])).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.contourf(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.gray)\n",
    "\n",
    "p = torch.tensor([[0.5],[0.5]], requires_grad=True)\n",
    "opt = optim.SGD([p], lr=0.01)\n",
    "\n",
    "path = np.empty((2,0))\n",
    "path = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "for i in range(50):\n",
    "    opt.zero_grad()\n",
    "    output = himm(p)\n",
    "    output.backward()\n",
    "    opt.step()\n",
    "    path = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "ax.plot(path[0], path[1], color='red', label='SGD', linewidth=2)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7281ca078d692fbe12ddfe12a0bd51c",
     "grade": false,
     "grade_id": "cell-f92c8fbe7bc00ba2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following block to run SGD with momentum (lr=0.01, momentum=0.9) from the same initial point, saving the position at each timestep into a variable called `path_mom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[0.5],[0.5]], requires_grad=True)\n",
    "opt = optim.SGD([p], lr=0.01, momentum=0.9)\n",
    "\n",
    "path = np.empty((2,0))\n",
    "path = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "for i in range(50):\n",
    "    opt.zero_grad()\n",
    "    output = himm(p)\n",
    "    output.backward()\n",
    "    opt.step()\n",
    "    path_mom = np.append(path, p.data.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f581993736855e0316c24d86dafede92",
     "grade": false,
     "grade_id": "cell-1a3d5f0a22670713",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following will plot the path taken when momentum was used, as well as the original plain SGD path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(path_mom[0], path_mom[1], color='yellow', label='SGDM', linewidth=2)\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ce2cd5108842b8589b0a7c0e789a54e",
     "grade": false,
     "grade_id": "cell-654d173df302005c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now explore what happens when you start from different points. What effect do you get with different optimisers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[0.5],[0.5]], requires_grad=True)\n",
    "opt = optim.Adam([p], lr=0.1)\n",
    "\n",
    "path = np.empty((2,0))\n",
    "path = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "for i in range(500):\n",
    "    opt.zero_grad()\n",
    "    output = himm(p)\n",
    "    output.backward()\n",
    "    opt.step()\n",
    "    path_adam = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "p = torch.tensor([[0.5],[0.5]], requires_grad=True)\n",
    "opt = optim.Adagrad([p], lr=0.1)\n",
    "\n",
    "path = np.empty((2,0))\n",
    "path = np.append(path, p.data.numpy(), axis=1)\n",
    "\n",
    "for i in range(10000):\n",
    "    opt.zero_grad()\n",
    "    output = himm(p)\n",
    "    output.backward()\n",
    "    opt.step()\n",
    "    path_ada = np.append(path, p.data.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(path_adam[0], path_adam[1], color='green', label='Adam', linewidth=2)\n",
    "ax.plot(path_ada[0], path_ada[1], color='blue', label='Ada', linewidth=2)\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_1_FuntionOptimisation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}